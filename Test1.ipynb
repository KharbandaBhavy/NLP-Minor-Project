{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from __future__ import print_function\n",
    "import array\n",
    "import string\n",
    "import operator\n",
    "\n",
    "# Natural Language Processing Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from flask import Flask, render_template, request #Used to render .html templates\n",
    "\n",
    "# Webscrapping using BeautifulSoup\n",
    "import bs4 as bs #beautifulsource4\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class summarize:\n",
    "\n",
    "\tdef get_summary(self, input, max_sentences):\n",
    "\t\tsentences_original = sent_tokenize(input)\n",
    "\n",
    "\t\t#Remove all tabs, and new lines\n",
    "\t\tif (max_sentences > len(sentences_original)):\n",
    "\t\t\tprint (\"Error, number of requested sentences exceeds number of sentences inputted\")\n",
    "\t\t\t#Should implement error schema to alert user.\n",
    "\t\ts = input.strip('\\t\\n')\n",
    "\t\t\n",
    "\t\t#Remove punctuation, tabs, new lines, and lowercase all words, then tokenize using words and sentences \n",
    "\t\twords_chopped = word_tokenize(s.lower())\n",
    "\t\t\n",
    "\t\tsentences_chopped = sent_tokenize(s.lower())\n",
    "\n",
    "\t\tstop_words = set(stopwords.words(\"english\"))\n",
    "\t\tpunc = set(string.punctuation)\n",
    "\n",
    "\t\t#Remove all stop words and punctuation from word list. \n",
    "\t\tfiltered_words = []\n",
    "\t\tfor w in words_chopped:\n",
    "\t\t\tif w not in stop_words and w not in punc:\n",
    "\t\t\t\tfiltered_words.append(w)\n",
    "\t\ttotal_words = len(filtered_words)\n",
    "\t\t\n",
    "\t\t#Determine the frequency of each filtered word and add the word and its frequency to a dictionary (key - word,value - frequency of that word)\n",
    "\t\tword_frequency = {}\n",
    "\t\toutput_sentence = []\n",
    "\n",
    "\t\tfor w in filtered_words:\n",
    "\t\t\tif w in word_frequency.keys():\n",
    "\t\t\t\tword_frequency[w] += 1.0 #increment the value: frequency\n",
    "\t\t\telse:\n",
    "\t\t\t\tword_frequency[w] = 1.0 #add the word to dictionary\n",
    "\n",
    "\t\t#Weighted frequency values - Assign weight to each word according to frequency and total words filtered from input:\n",
    "\t\tfor word in word_frequency:\n",
    "\t\t\tword_frequency[word] = (word_frequency[word]/total_words)\n",
    "\n",
    "\t\t#Keep a tracker for the most frequent words that appear in each sentence and add the sum of their weighted frequency values. \n",
    "\t\t#Note: Each tracker index corresponds to each original sentence.\n",
    "\t\ttracker = [0.0] * len(sentences_original)\n",
    "\t\tfor i in range(0, len(sentences_original)):\n",
    "\t\t\tfor j in word_frequency:\n",
    "\t\t\t\tif j in sentences_original[i]:\n",
    "\t\t\t\t\ttracker[i] += word_frequency[j]\n",
    "\n",
    "\t\t#Get the highest weighted sentence and its index from the tracker. We take those and output the associated sentences.\n",
    "\t\t\n",
    "\t\tfor i in range(0, len(tracker)):\n",
    "\t\t\t\n",
    "\t\t\t#Extract the index with the highest weighted frequency from tracker\n",
    "\t\t\tindex, value = max(enumerate(tracker), key = operator.itemgetter(1))\n",
    "\t\t\tif (len(output_sentence)+1 <= max_sentences) and (sentences_original[index] not in output_sentence): \n",
    "\t\t\t\toutput_sentence.append(sentences_original[index])\n",
    "\t\t\tif len(output_sentence) > max_sentences:\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\t#Remove that sentence from the tracker, as we will take the next highest weighted freq in next iteration\n",
    "\t\t\ttracker.remove(tracker[index])\n",
    "\t\t\n",
    "\t\tsorted_output_sent = self.sort_sentences(sentences_original, output_sentence)\n",
    "\t\treturn (sorted_output_sent)\n",
    "\n",
    "\t# @def sort_senteces:\n",
    "\t# From the output sentences, sort them such that they appear in the order the input text was provided.\n",
    "\t# Makes it flow more with the theme of the story/article etc..\n",
    "\tdef sort_sentences (self, original, output):\n",
    "\t\tsorted_sent_arr = []\n",
    "\t\tsorted_output = []\n",
    "\t\tfor i in range(0, len(output)):\n",
    "\t\t\tif(output[i] in original):\n",
    "\t\t\t\tsorted_sent_arr.append(original.index(output[i]))\n",
    "\t\tsorted_sent_arr = sorted(sorted_sent_arr)\n",
    "\n",
    "\t\tfor i in range(0, len(sorted_sent_arr)):\n",
    "\t\t\tsorted_output.append(original[sorted_sent_arr[i]])\n",
    "\t\tprint (sorted_sent_arr)\n",
    "\t\treturn sorted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#------------Flask Application---------------#\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/templates', methods=['POST'])\n",
    "def original_text_form():\n",
    "\ttitle = \"Summarizer\"\n",
    "\ttext = request.form['input_text'] #Get text from html\n",
    "\tmax_value = sent_tokenize(text)\n",
    "\tnum_sent = int(request.form['num_sentences']) #Get number of sentence required in summary\n",
    "\tsum1 = summarize()\n",
    "\tsummary = sum1.get_summary(text, num_sent)\n",
    "\tprint (summary)\n",
    "\treturn render_template(\"index.html\", title = title, original_text = text, output_summary = summary, num_sentences = max_value)\n",
    "@app.route('/')\n",
    "def homepage():\n",
    "\ttitle = \"Text Summarizer\"\n",
    "\treturn render_template(\"index.html\", title = title)\n",
    "\t\n",
    "if __name__ == \"__main__\":\n",
    "\tapp.debug = True\n",
    "\tapp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ee691e3c2883c6a130fe7a4fe904a1eaf00aeea2af805a016ed0d0bd8e74479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
